{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "esNWMzKrVuWc"
      },
      "source": [
        "**Revised on 3/5/2024: Changed source files**\n",
        "\n",
        "This is the skeleton code for Task 1 of the midterm project. The files that are downloaded in step 4 are based on the [Ember 2018 dataset](https://arxiv.org/abs/1804.04637), and contain the features (and corresponding labels) extracted from 1 million PE files, split into 80\\% training and 20\\% test datasets. The code used for for feature extraction is available [here](https://colab.research.google.com/drive/16q9bOlCmnTquPtVXVzxUj4ZY1ORp10R2?usp=sharing). However, the preprocessing and featurization process may take up to 3 hours on Google Colab. Hence, I recommend using the processed datasets (Step 4) to speed up your development.\n",
        "\n",
        "Also, note that there is a new optional step 8.5 - To speed up your experiments, you may want to sample the original dataset of 800k training samples and 200k test samples to smaller datasets.\n",
        "\n",
        "**Step 1:** Mount your Google Drive by clicking on \"Mount Drive\" in the Files section (panel to the left of this text.)\n",
        "\n",
        "**Step 2:** Go to Runtime -> Change runtime type and select T4 GPU.\n",
        "\n",
        "**Step 3:** Create a folder in your Google Drive, and rename it to \"vMalConv\"\n",
        "\n",
        "**Step 4:** Download the pre-processed training and test datasets."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eUq_FZwmZegw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b4ada590-a2f6-481c-a7ad-5cdc6c223f7d"
      },
      "source": [
        "# ~8GB\n",
        "!wget https://dsci6015s24-midterm.s3.amazonaws.com/v2/X_train.dat\n",
        "!wget https://dsci6015s24-midterm.s3.amazonaws.com/v2/X_test.dat\n",
        "!wget https://dsci6015s24-midterm.s3.amazonaws.com/v2/y_train.dat\n",
        "!wget https://dsci6015s24-midterm.s3.amazonaws.com/v2/y_test.dat\n",
        "!wget https://dsci6015s24-midterm.s3.amazonaws.com/v2/metadata.csv"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-03-12 18:53:00--  https://dsci6015s24-midterm.s3.amazonaws.com/v2/X_train.dat\n",
            "Resolving dsci6015s24-midterm.s3.amazonaws.com (dsci6015s24-midterm.s3.amazonaws.com)... 52.216.54.9, 3.5.28.203, 52.217.165.89, ...\n",
            "Connecting to dsci6015s24-midterm.s3.amazonaws.com (dsci6015s24-midterm.s3.amazonaws.com)|52.216.54.9|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 7619200000 (7.1G) [binary/octet-stream]\n",
            "Saving to: ‘X_train.dat’\n",
            "\n",
            "X_train.dat         100%[===================>]   7.10G  49.3MB/s    in 2m 27s  \n",
            "\n",
            "2024-03-12 18:55:28 (49.4 MB/s) - ‘X_train.dat’ saved [7619200000/7619200000]\n",
            "\n",
            "--2024-03-12 18:55:28--  https://dsci6015s24-midterm.s3.amazonaws.com/v2/X_test.dat\n",
            "Resolving dsci6015s24-midterm.s3.amazonaws.com (dsci6015s24-midterm.s3.amazonaws.com)... 3.5.3.100, 54.231.138.185, 54.231.236.193, ...\n",
            "Connecting to dsci6015s24-midterm.s3.amazonaws.com (dsci6015s24-midterm.s3.amazonaws.com)|3.5.3.100|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1904800000 (1.8G) [binary/octet-stream]\n",
            "Saving to: ‘X_test.dat’\n",
            "\n",
            "X_test.dat          100%[===================>]   1.77G  52.0MB/s    in 39s     \n",
            "\n",
            "2024-03-12 18:56:07 (46.9 MB/s) - ‘X_test.dat’ saved [1904800000/1904800000]\n",
            "\n",
            "--2024-03-12 18:56:07--  https://dsci6015s24-midterm.s3.amazonaws.com/v2/y_train.dat\n",
            "Resolving dsci6015s24-midterm.s3.amazonaws.com (dsci6015s24-midterm.s3.amazonaws.com)... 3.5.3.11, 3.5.28.199, 54.231.198.89, ...\n",
            "Connecting to dsci6015s24-midterm.s3.amazonaws.com (dsci6015s24-midterm.s3.amazonaws.com)|3.5.3.11|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 3200000 (3.1M) [binary/octet-stream]\n",
            "Saving to: ‘y_train.dat’\n",
            "\n",
            "y_train.dat         100%[===================>]   3.05M  14.4MB/s    in 0.2s    \n",
            "\n",
            "2024-03-12 18:56:07 (14.4 MB/s) - ‘y_train.dat’ saved [3200000/3200000]\n",
            "\n",
            "--2024-03-12 18:56:07--  https://dsci6015s24-midterm.s3.amazonaws.com/v2/y_test.dat\n",
            "Resolving dsci6015s24-midterm.s3.amazonaws.com (dsci6015s24-midterm.s3.amazonaws.com)... 3.5.3.11, 3.5.28.199, 54.231.198.89, ...\n",
            "Connecting to dsci6015s24-midterm.s3.amazonaws.com (dsci6015s24-midterm.s3.amazonaws.com)|3.5.3.11|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 800000 (781K) [binary/octet-stream]\n",
            "Saving to: ‘y_test.dat’\n",
            "\n",
            "y_test.dat          100%[===================>] 781.25K  --.-KB/s    in 0.1s    \n",
            "\n",
            "2024-03-12 18:56:08 (5.15 MB/s) - ‘y_test.dat’ saved [800000/800000]\n",
            "\n",
            "--2024-03-12 18:56:08--  https://dsci6015s24-midterm.s3.amazonaws.com/v2/metadata.csv\n",
            "Resolving dsci6015s24-midterm.s3.amazonaws.com (dsci6015s24-midterm.s3.amazonaws.com)... 3.5.3.11, 3.5.28.199, 54.231.198.89, ...\n",
            "Connecting to dsci6015s24-midterm.s3.amazonaws.com (dsci6015s24-midterm.s3.amazonaws.com)|3.5.3.11|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 92160330 (88M) [text/csv]\n",
            "Saving to: ‘metadata.csv’\n",
            "\n",
            "metadata.csv        100%[===================>]  87.89M  53.9MB/s    in 1.6s    \n",
            "\n",
            "2024-03-12 18:56:10 (53.9 MB/s) - ‘metadata.csv’ saved [92160330/92160330]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AQ_JdZKfG7Q-",
        "outputId": "d7726081-7d55-4eba-b7f9-7418f1d60dbb"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9V958PbDW3H0"
      },
      "source": [
        "**Step 5:** Copy the downloaded files to vMalConv"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "llip77F3amma"
      },
      "source": [
        "!cp /content/X_train.dat /content/drive/MyDrive/TModel/X_train.dat\n",
        "!cp /content/X_test.dat /content/drive/MyDrive/TModel/X_test.dat\n",
        "!cp /content/y_train.dat /content/drive/MyDrive/TModel/y_train.dat\n",
        "!cp /content/y_test.dat /content/drive/MyDrive/TModel/y_test.dat\n",
        "!cp /content/metadata.csv /content/drive/MyDrive/TModel/metadata.csv"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QbRilyqTXnrE"
      },
      "source": [
        "**Step 6:** Download and install Ember:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "76bc7PEmlwKB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cf58afe6-4bbf-4a0e-b080-a15e18240b48"
      },
      "source": [
        "!pip install git+https://github.com/PFGimenez/ember.git"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting git+https://github.com/PFGimenez/ember.git\n",
            "  Cloning https://github.com/PFGimenez/ember.git to /tmp/pip-req-build-9m14wrgd\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/PFGimenez/ember.git /tmp/pip-req-build-9m14wrgd\n",
            "  Resolved https://github.com/PFGimenez/ember.git to commit 3b82fe63069884882e743af725d29cc2a67859f1\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: ember\n",
            "  Building wheel for ember (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for ember: filename=ember-0.1.0-py3-none-any.whl size=13050 sha256=5bc8861a8ee6b34eb556b94340ec3853d926d8743ee3d3921e66c3adf1f0322d\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-8oi_lkre/wheels/8f/69/f9/1917c8df03b25fe53e8e2f6cb2c9f61a43dec179b19b10ab9f\n",
            "Successfully built ember\n",
            "Installing collected packages: ember\n",
            "Successfully installed ember-0.1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install lief"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8aRVMSwCQT7D",
        "outputId": "3753e4ad-ed27-484a-84dd-b268560c0d85"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting lief\n",
            "  Downloading lief-0.14.1-cp310-cp310-manylinux_2_28_x86_64.manylinux_2_27_x86_64.whl (2.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.7/2.7 MB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: lief\n",
            "Successfully installed lief-0.14.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AXym5qd8Yv8f"
      },
      "source": [
        "**Step 7:** Read vectorized features from the data files."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GfcHyoTsmCFH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "88c5e13d-89a3-4e36-9462-7a62f67a0742"
      },
      "source": [
        "import ember\n",
        "X_train, y_train, X_test, y_test = ember.read_vectorized_features(\"drive/MyDrive/TModel/\")\n",
        "metadata_dataframe = ember.read_metadata(\"drive/MyDrive/TModel/\")"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING: EMBER feature version 2 were computed using lief version 0.9.0-\n",
            "WARNING:   lief version 0.14.1-bae887e0 found instead. There may be slight inconsistencies\n",
            "WARNING:   in the feature calculations.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FTRCz7m7Z7EH"
      },
      "source": [
        "**Step 8:** Get rid of rows with no labels."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zj63lcvin44q"
      },
      "source": [
        "labelrows = (y_train != -1)\n",
        "X_train = X_train[labelrows]\n",
        "y_train = y_train[labelrows]"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mVG59AGooyC5"
      },
      "source": [
        "import h5py\n",
        "h5f = h5py.File('X_train.h5', 'w')\n",
        "h5f.create_dataset('X_train', data=X_train)\n",
        "h5f.close()\n",
        "h5f = h5py.File('y_train.h5', 'w')\n",
        "h5f.create_dataset('y_train', data=y_train)\n",
        "h5f.close()"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!cp /content/X_train.h5 /content/drive/MyDrive/TModel/X_train.h5\n",
        "!cp /content/y_train.h5 /content/drive/MyDrive/TModel/y_train.h5"
      ],
      "metadata": {
        "id": "oc2_4D5Kbeyt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!cp /content/y_train.h5 /content/drive/MyDrive/TModel/sampled_y_train.h5\n",
        "!cp /content/X_train.h5 /content/drive/MyDrive/TModel/sampled_x_train.h5"
      ],
      "metadata": {
        "id": "-ynwMQmKDPhf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Optional Step 8.5:** To speed up your experiments, you may want to sample the original dataset of 800k training samples and 200k test samples to smaller datasets. You can use the [Pandas Dataframe sample() method](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.sample.html), or come up with your own sampling methodology. Be mindful of the fact that the database is heavily imbalanced."
      ],
      "metadata": {
        "id": "tKoXSzp59RN-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import h5py\n",
        "\n",
        "# Converting numpy arrays to Pandas DataFrames\n",
        "X_train_df = pd.DataFrame(X_train)\n",
        "y_train_df = pd.DataFrame(y_train)\n",
        "X_test_df = pd.DataFrame(X_test)\n",
        "y_test_df = pd.DataFrame(y_test)\n",
        "\n",
        "# Sampling only a portion of the original dataset to create a smaller dataset\n",
        "sample_size = 75000\n",
        "\n",
        "# Sampling the training dataset\n",
        "sampled_X_train = X_train_df.sample(n=sample_size, random_state=1)\n",
        "sampled_y_train = y_train_df.loc[X_train_df.index.isin(sampled_X_train.index)]\n",
        "\n",
        "# Sampling the test dataset\n",
        "sampled_X_test = X_test_df.sample(n=sample_size, random_state=1)\n",
        "sampled_y_test = y_test_df.loc[X_test_df.index.isin(sampled_X_test.index)]\n",
        "\n",
        "# Saving the sampled datasets to h5 files\n",
        "h5f = h5py.File('sampled_X_train.h5', 'w')\n",
        "h5f.create_dataset('sampled_X_train', data=sampled_X_train)\n",
        "h5f.close()\n",
        "\n",
        "h5f = h5py.File('sampled_y_train.h5', 'w')\n",
        "h5f.create_dataset('sampled_y_train', data=sampled_y_train)\n",
        "h5f.close()\n",
        "\n",
        "h5f = h5py.File('sampled_X_test.h5', 'w')\n",
        "h5f.create_dataset('sampled_X_test', data=sampled_X_test)\n",
        "h5f.close()\n",
        "\n",
        "h5f = h5py.File('sampled_y_test.h5', 'w')\n",
        "h5f.create_dataset('sampled_y_test', data=sampled_y_test)\n",
        "h5f.close()"
      ],
      "metadata": {
        "id": "-X9wwv_n9QkY"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!cp /content/sampled_X_test.h5 /content/drive/MyDrive/TModel/sampled_X_test.h5\n",
        "!cp /content/sampled_X_train.h5 /content/drive/MyDrive/TModel/sampled_X_train.h5\n",
        "!cp /content/sampled_y_train.h5 /content/drive/MyDrive/TModel/sampled_y_train.h5\n",
        "!cp /content/sampled_y_test.h5 /content/drive/MyDrive/TModel/sampled_y_test.h5"
      ],
      "metadata": {
        "id": "XizqbhJCyc43"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F1bRlBWlaQdd"
      },
      "source": [
        "> **Task 1:** Complete the following code to build the architecture of MalConv in PyTorch:"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import h5py\n",
        "\n",
        "# Loading the sampled training data from Colab environment\n",
        "h5f = h5py.File('/content/drive/MyDrive/TModel/sampled_X_train.h5', 'r')\n",
        "sampled_X_train_tensor = torch.tensor(h5f['sampled_X_train'][:], dtype=torch.long)\n",
        "h5f.close()\n",
        "\n",
        "# Define the MalConv model class\n",
        "class MalConv(nn.Module):\n",
        "    def __init__(self, input_length=2000000, embedding_dim=8, window_size=500, output_dim=1):\n",
        "        super(MalConv, self).__init__()\n",
        "        self.embed = nn.Embedding(256, embedding_dim, padding_idx=0)  # Embedding layer for 256 unique bytes with specified dimension\n",
        "        self.conv1 = nn.Conv1d(in_channels=embedding_dim, out_channels=128, kernel_size=32, stride=32)\n",
        "        self.conv2 = nn.Conv1d(in_channels=128, out_channels=128, kernel_size=32, stride=32)\n",
        "        self.fc1 = nn.Linear(128, 128)\n",
        "        self.fc2 = nn.Linear(128, output_dim)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.embed(x.clamp(min=0, max=255))  # Clamping indices to ensure they're within the valid range before embedding\n",
        "        x = x.transpose(1, 2)  # Adjusting dimensions for Conv1d\n",
        "        x = self.conv1(x)\n",
        "        x = torch.relu(x)\n",
        "        x = self.conv2(x)\n",
        "        x = torch.relu(x)\n",
        "        x = torch.squeeze(torch.max(x, dim=2)[0])  # Global max pooling\n",
        "        x = self.fc1(x)\n",
        "        x = torch.relu(x)\n",
        "        x = self.fc2(x)\n",
        "        x = self.sigmoid(x)\n",
        "        return x\n",
        "\n",
        "# Example input using the sampled training data\n",
        "input_length = 2000000  # Fixed length for each input file\n",
        "batch_size = 4\n",
        "example_input = sampled_X_train_tensor[:batch_size, :input_length]  # Using the first 4 examples from sampled_X_train\n",
        "\n",
        "# Creating and initializing the MalConv model\n",
        "model = MalConv(input_length=input_length, embedding_dim=8, window_size=500, output_dim=1)\n",
        "\n",
        "# Using the MalConv model for predictions\n",
        "output = model(example_input)\n",
        "print(output)  # Displaying the output probabilities for each example\n"
      ],
      "metadata": {
        "id": "O0pEuSzDHEh9",
        "outputId": "ca92718e-2064-4c0d-e8a0-86a753ebb526",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[0.4864],\n",
            "        [0.4845],\n",
            "        [0.4859],\n",
            "        [0.4843]], grad_fn=<SigmoidBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pihnLcFmbaet"
      },
      "source": [
        "**Step 8:** Partial fit the standardScaler to avoid overloading the memory:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H4q5OfK9v9iN"
      },
      "source": [
        "import h5py\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Load sampled_X_train\n",
        "drive_path = '/content/drive/MyDrive/TModel/sampled_X_train.h5'\n",
        "with h5py.File(drive_path, 'r') as hf:\n",
        "    sampled_X_train = hf['sampled_X_train'][:]\n",
        "\n",
        "\n",
        "mms = StandardScaler()\n",
        "for x in range(0, len(sampled_X_train), 100000):\n",
        "    mms.partial_fit(sampled_X_train[x:x+100000])"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B33Oa1sTxdB0"
      },
      "source": [
        "sampled_X_train = mms.transform(sampled_X_train)"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V_vl5yrex0yY"
      },
      "source": [
        "import h5py\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Load sampled_X_train\n",
        "drive_path = '/content/drive/MyDrive/TModel/sampled_X_train.h5'\n",
        "with h5py.File(drive_path, 'r') as hf:\n",
        "    sampled_X_train = hf['sampled_X_train'][:]\n",
        "\n",
        "\n",
        "mms = StandardScaler()\n",
        "for x in range(0, len(sampled_X_train), 100000):\n",
        "    mms.partial_fit(sampled_X_train[x:x+100000])"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Load, Tensorize, and Split** The following code takes care of converting the training data into Torch Tensors, and then splits it into 80% training and 20% validation datasets."
      ],
      "metadata": {
        "id": "b1iRXFtuvCps"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import h5py\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Read sampled_y_train\n",
        "drive_path_sampled_y_train = '/content/drive/MyDrive/TModel/sampled_y_train.h5'\n",
        "with h5py.File(drive_path_sampled_y_train, 'r') as hf:\n",
        "    sampled_y_train = hf['sampled_y_train'][:]\n",
        "\n",
        "# Load sampled_X_train\n",
        "drive_path = '/content/drive/MyDrive/TModel/sampled_X_train.h5'\n",
        "with h5py.File(drive_path, 'r') as hf:\n",
        "    sampled_X_train = hf['sampled_X_train'][:]\n",
        "\n",
        "# Reshape the data if necessary\n",
        "# Assuming the data is 2D, adjust the reshape accordingly\n",
        "sampled_X_train = sampled_X_train.reshape(-1, 1)\n",
        "\n",
        "# Scale the input data\n",
        "scaler = StandardScaler()\n",
        "sampled_X_train_scaled = scaler.fit_transform(sampled_X_train)\n",
        "\n",
        "# Convert your numpy arrays to PyTorch tensors\n",
        "sampled_X_train_tensor = torch.tensor(sampled_X_train_scaled, dtype=torch.float32)\n",
        "sampled_y_train_tensor = torch.tensor(sampled_y_train, dtype=torch.float32)\n",
        "\n",
        "# Split the data into training and validation sets (80% training, 20% validation)\n",
        "sampled_X_train_tensor, sampled_y_train_tensor = sampled_X_train_tensor[:len(sampled_y_train_tensor)], sampled_y_train_tensor\n",
        "sampled_X_train_split, sampled_X_val_split, sampled_y_train_split, sampled_y_val_split = train_test_split(\n",
        "    sampled_X_train_tensor, sampled_y_train_tensor, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# Create TensorDatasets and DataLoaders for training and validation sets\n",
        "sampled_train_dataset = TensorDataset(sampled_X_train_split, sampled_y_train_split)\n",
        "sampled_val_dataset = TensorDataset(sampled_X_val_split, sampled_y_val_split)\n",
        "\n",
        "batch_size = 16\n",
        "sampled_train_loader = DataLoader(sampled_train_dataset, batch_size=batch_size, shuffle=True)\n",
        "sampled_val_loader = DataLoader(sampled_val_dataset, batch_size=batch_size, shuffle=False)\n"
      ],
      "metadata": {
        "id": "Ja3fhJI6qJKN"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2zMgth6McCqV"
      },
      "source": [
        "> **Task 2:** Complete the following code to train the model on the GPU for 15 epochs, with a batch size of 64. If you are on Google Colab, don't forget to change the kernel in Runtime -> Change runtime type -> T4 GPU."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n"
      ],
      "metadata": {
        "id": "nFusDP-npEiZ"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "import os\n",
        "\n",
        "# Assuming MalConv class definition is already provided as above\n",
        "\n",
        "# Initialize the MalConv model\n",
        "model = MalConv()\n",
        "\n",
        "# Move model to GPU if available\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "\n",
        "# Loss function and optimizer\n",
        "criterion = nn.BCELoss()  # Binary Cross-Entropy Loss for binary classification\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)  # Adjust learning rate as needed\n",
        "\n",
        "# Directory to save model checkpoints\n",
        "save_dir = \"/content\"\n",
        "\n",
        "# Training Loop with Validation\n",
        "num_epochs = 15  # Adjust the number of epochs as needed\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()  # Set model to training mode\n",
        "    running_loss = 0.0\n",
        "\n",
        "    for inputs, labels in sampled_train_loader:\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "        # Zero the parameter gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs.squeeze(), labels.squeeze())\n",
        "\n",
        "        # Backward pass and optimize\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "\n",
        "    print(f'Epoch {epoch+1}, Training Loss: {running_loss/len(sampled_train_loader)}')\n",
        "\n",
        "    # Validation step\n",
        "    model.eval()  # Set model to evaluation mode\n",
        "    val_loss = 0.0\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in sampled_val_loader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs.squeeze(), labels.squeeze())\n",
        "            val_loss += loss.item()\n",
        "    print(f'Validation Loss: {val_loss/len(sampled_val_loader)}')\n",
        "\n",
        "    # Save checkpoint every 5 epochs\n",
        "    if (epoch + 1) % 5 == 0:\n",
        "        checkpoint_path = os.path.join(save_dir, f'model_epoch_{epoch+1}.pt')\n",
        "        torch.save(model.state_dict(), checkpoint_path)\n",
        "        print(f'Model checkpoint saved to {checkpoint_path}')\n"
      ],
      "metadata": {
        "id": "tkfZ08FkYMbS",
        "outputId": "1f160e9c-b8f3-44d7-9b36-ffca0553aacf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 400
        }
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "Expected tensor for argument #1 'indices' to have one of the following scalar types: Long, Int; but got torch.FloatTensor instead (while checking arguments for embedding)",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-22-df79de2bc703>\u001b[0m in \u001b[0;36m<cell line: 26>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[0;31m# Forward pass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1517\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1518\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1519\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1520\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1525\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1526\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1528\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1529\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-6-7b6e997aea65>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclamp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmin\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m255\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Clamping indices to ensure they're within the valid range before embedding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Adjusting dimensions for Conv1d\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1517\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1518\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1519\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1520\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1525\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1526\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1528\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1529\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/sparse.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    160\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    161\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 162\u001b[0;31m         return F.embedding(\n\u001b[0m\u001b[1;32m    163\u001b[0m             \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpadding_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_norm\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    164\u001b[0m             self.norm_type, self.scale_grad_by_freq, self.sparse)\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36membedding\u001b[0;34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[0m\n\u001b[1;32m   2231\u001b[0m         \u001b[0;31m# remove once script supports set_grad_enabled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2232\u001b[0m         \u001b[0m_no_grad_embedding_renorm_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_norm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnorm_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2233\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscale_grad_by_freq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msparse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2234\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2235\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Expected tensor for argument #1 'indices' to have one of the following scalar types: Long, Int; but got torch.FloatTensor instead (while checking arguments for embedding)"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cp /content/drive/MyDrive/vMalConv/model_epoch_5.pt /content/drive/MyDrive/vMalConv1/model_epoch_5.pt\n",
        "!cp /content/drive/MyDrive/vMalConv/model_epoch_10.pt /content/drive/MyDrive/vMalConv1/model_epoch_10.pt\n",
        "!cp /content/drive/MyDrive/vMalConv/model_epoch_15.pt /content/drive/MyDrive/vMalConv1/model_epoch_15.pt\n"
      ],
      "metadata": {
        "id": "MeEDbR4a8xUy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Task 3:** Complete the following code to evaluate your trained model on the test data."
      ],
      "metadata": {
        "id": "obToo1WZtD4m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score, precision_score, recall_score\n",
        "\n",
        "# Reading sampled_y_test\n",
        "drive_path_sampled_y_test = '/content/drive/MyDrive/vMalConv1/sampled_y_test.h5'\n",
        "with h5py.File(drive_path_sampled_y_test, 'r') as hf:\n",
        "    sampled_y_test = hf['sampled_y_test'][:]\n",
        "\n",
        "# Loading sampled_X_test\n",
        "drive_path_sampled_X_test = '/content/drive/MyDrive/vMalConv1/sampled_X_test.h5'\n",
        "with h5py.File(drive_path_sampled_X_test, 'r') as hf:\n",
        "    sampled_X_test = hf['sampled_X_test'][:]\n",
        "\n",
        "# Converting test data to PyTorch tensors\n",
        "sampled_X_test_tensor = torch.tensor(sampled_X_test, dtype=torch.long)\n",
        "sampled_y_test_tensor = torch.tensor(sampled_y_test, dtype=torch.float32)\n",
        "\n",
        "# Creating a TensorDataset and DataLoader for test data\n",
        "sampled_test_dataset = TensorDataset(sampled_X_test_tensor, sampled_y_test_tensor)\n",
        "sampled_test_loader = DataLoader(sampled_test_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "# Ensuring the model is in evaluation mode\n",
        "model.eval()\n",
        "\n",
        "# Lists to store model predictions and actual labels\n",
        "predictions = []\n",
        "labels = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for inputs, labels_batch in sampled_test_loader:\n",
        "        inputs, labels_batch = inputs.to(device), labels_batch.to(device)\n",
        "        outputs = model(inputs)\n",
        "        predicted = torch.round(outputs)  # Rounding to 0 or 1\n",
        "        predictions.extend(predicted.cpu().numpy())\n",
        "        labels.extend(labels_batch.cpu().numpy())\n",
        "\n",
        "# Computing metrics\n",
        "accuracy = accuracy_score(labels, predictions)\n",
        "precision = precision_score(labels, predictions)\n",
        "recall = recall_score(labels, predictions)\n",
        "\n",
        "print(f'Test Accuracy: {accuracy:.4f}')\n",
        "print(f'Precision: {precision:.4f}')\n",
        "print(f'Recall: {recall:.4f}')\n"
      ],
      "metadata": {
        "id": "eXJ-lDb1YeGr",
        "outputId": "a987f563-be4b-44b7-d7cc-48a95f128053",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Accuracy: 0.5077\n",
            "Precision: 0.5068\n",
            "Recall: 0.5950\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Task 4:** Comment on the results in this text box.\n",
        "\n",
        "\n",
        "The overall evaluation of the model's performance might not be comprehensive due to the utilization of a small sample size of 75,000 instances. This size may not adequately reflect how effectively the model performs across all classes, particularly in datasets with imbalanced class distributions.\n",
        "\n",
        "Precision and Recall: These metrics serve as complementary measures. High precision signifies a low false positive rate, indicating that when the model predicts a class, it's likely to be correct. On the other hand, high recall implies a low false negative rate, suggesting that the model can effectively capture instances of the positive class. However, it's important to note that a high recall might result in a higher number of false positives, potentially leading to a situation where benign instances are incorrectly classified as malware.\n",
        "\n",
        "Training Loss: This metric represents the error incurred during the training phase of the model. It indicates how well the model is fitting the training data. Lower training loss values generally imply that the model is effectively learning the patterns in the training data.\n",
        "\n",
        "Validation Loss: This metric measures the error on a separate validation dataset that the model hasn't seen during training. It helps assess how well the model generalizes to unseen data. A low validation loss indicates that the model is performing well not only on the training data but also on new, unseen instances.\n",
        "\n",
        "Combining Precision, Recall, Training Loss, and Validation Loss provides a comprehensive understanding of the model's performance, considering both its predictive accuracy and generalization capabilities."
      ],
      "metadata": {
        "id": "W6fLYYxps91N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "3kf5qNWO_FvA"
      }
    }
  ]
}